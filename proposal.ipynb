{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Wildfire Analysis\n",
    "## Michael Peters and Nick Latham\n",
    "CPSC322\n",
    "04.06.21"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Sources\n",
    "\n",
    "1970-2007 WADNR Wildfire Data:\n",
    "\n",
    "https://data-wadnr.opendata.arcgis.com/datasets/dnr-fire-statistics-1970-2007-1\n",
    "\n",
    "\n",
    "2008-2019 WADNR Wildfire Data:\n",
    "\n",
    "https://data-wadnr.opendata.arcgis.com/datasets/dnr-fire-statistics-2008-present-1"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Source Description\n",
    "\n",
    "Both data sets come in the form of CSV or GeoJson files. For our application, CSV files will most likely be easier, but we get more data from GeoJson.\n",
    "\n",
    "The most interesting/valuable trait that we could predict is the size of a future fire. We plan to use a discretization of various fire sizes to create a 1-10 level chart similar to the MPG chart. The attribute that currently holds data for the size of a fire is \"ACRES_BURNED\". When looking at the two datasets, you can see that the 2008-Present dataset has many more attributes."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Implementation\n",
    "\n",
    "These files are quite large. This may be a challenge for sifting through the data to remove incomplete entries. We have nearly 60,000 independent fire events that have been logged by the Washington Department of Natural Resources for the last 50 years. I anticipate strong results with a dataset this size, and with enough effort, we will have a dataset that is fit for distribution at the end of the project.\n",
    "\n",
    "Given that we are trying to predict a binned value, the best thing to do would be to check for correlation between other attributes, and then build our attribute list from attributes with strong correlation. We want to avoid perfect multicollinearity, and with a dataset this size, it is improbable that that would happen. From a cursory glance, potentially useful attributes will be County, Fire Cause, and Date. One unique challenge we face is that there are many more attributes from 2008 onward, but we would also like the historical benefit of having 50 years worth of data. It may be necessary to create default values to substitute for missing values, instead of just removing the entry."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Potential Impact\n",
    "\n",
    "This project was first attempted at \"https://github.com/michaelpeterswa/kulo\" where location was used to find a continuous output value, using Keras and Tensorflow. That project had mixed results and the model was not well refined, which lead to inconclusive outputs. I (Michael) hope to expand upon that by getting useful predictions that work for future wildfire events.\n",
    "\n",
    "These results could be useful for a number of purposes. The first obvious choice would be predicting how at-risk a property is to wildfires in the State of Washington. This metric would be useful to homeowners, insurance underwriters, fire departments, and city planners. Of course, this project won't truly be production ready in the time-frame we have, but it's good to look forward at future potentials."
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}